I"G<ul id="markdown-toc">
  <li><a href="#连续型概率分布" id="markdown-toc-连续型概率分布">连续型概率分布</a>    <ul>
      <li><a href="#正态分布" id="markdown-toc-正态分布">正态分布</a>        <ul>
          <li><a href="#中心极限定理" id="markdown-toc-中心极限定理">中心极限定理</a></li>
          <li><a href="#大数定律" id="markdown-toc-大数定律">大数定律</a></li>
        </ul>
      </li>
      <li><a href="#幂律分布power-law-distribution" id="markdown-toc-幂律分布power-law-distribution">幂律分布（Power law distribution）</a></li>
    </ul>
  </li>
  <li><a href="#贝叶斯定理的启示" id="markdown-toc-贝叶斯定理的启示">贝叶斯定理的启示</a></li>
  <li><a href="#数理统计" id="markdown-toc-数理统计">数理统计</a>    <ul>
      <li><a href="#两种方法" id="markdown-toc-两种方法">两种方法</a>        <ul>
          <li><a href="#参数估计点估计区间估计" id="markdown-toc-参数估计点估计区间估计">参数估计(点估计/区间估计),</a></li>
          <li><a href="#假设检验" id="markdown-toc-假设检验">假设检验</a></li>
          <li><a href="#sig2的无偏估计量" id="markdown-toc-sig2的无偏估计量">sig^2的无偏估计量</a></li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h1 id="连续型概率分布">连续型概率分布</h1>
<h3 id="正态分布">正态分布</h3>
<p>一般来说，如果一个量是由许多微小的独立随机因素影响的结果，那么就可以认为这个量具有正态分布。从理论上看，正态分布具有很多良好的性质，许多概率分布可以用它来近似；还有一些常用的概率分布是由它直接导出的，例如对数正态分布、t分布、F分布等。</p>
<h5 id="中心极限定理">中心极限定理</h5>
<p>中心极限定理是说，当样本量N逐渐趋于无穷大时，N个抽样样本的均值的频数逐渐趋于正态分布, 并且这个正态分布以u为均值，sigma^2/n为方差。</p>

<h5 id="大数定律">大数定律</h5>
<p>大数定律是说，n只要越来越大，我把这n个独立同分布的数加起来去除以n得到的这个样本均值（也是一个随机变量）会依概率收敛到真值u，但是样本均值的分布是怎样的我们不知道。</p>

<p>综上所述，这两个定律都是在说样本均值性质。随着n增大，大数定律说样本均值几乎必然等于均值。中心极限定律说，他越来越趋近于正态分布。并且这个正态分布的方差越来越小。</p>

<h3 id="幂律分布power-law-distribution">幂律分布（Power law distribution）</h3>
<p>幂律分布表现为斜率为负的幂指数的直线，概率越高，占比越小，生活中的马太效应及长尾分布都是幂律分布的典型案例。简单来说，幂律分布就是告诉你世界是不公平的，较少的人拥有大部分财富（二八法则），于是在他们身上就体现了财富的加速积累情况。</p>

<h1 id="贝叶斯定理的启示">贝叶斯定理的启示</h1>
<p>有了新情况，我原来的看法会改变，新情况和自己的预期一致就强化原来的看法，否则就弱化，这不就是常识吗，还用得着什么数学定理吗？</p>

<p>拉普拉斯说过，所谓的概率就是把人们的常识用数学表达出来。也有人说，人脑就是采用贝叶斯方法来工作的。</p>

<p>但是我们人脑有偏差啊，有误区啊，会犯浑啊，这个公式让我们忽然获得了一个上帝视角，来审视一下，我们自己究竟是怎么做判断，做决定的，计算机又是怎么模仿并超越我们的，这岂不是很美妙的一件事情</p>

<p>贝叶斯定理给我们的启示
塔勒布说过，数学不仅仅是计算，而是一种思考方式。</p>

<p>现实世界中，我们没法时时刻刻拿出电脑来演算一下公式，但是我们仍然可以通过这个定理得到一些宝贵的启示：</p>

<p>1、先行动起来。</p>

<p>大胆假设，小心求证。不断调整，快速迭代。这就是贝叶斯方法。</p>

<p>当信息不完备时，对概率的判断没有把握时，当然可以选择以静制动，但是不行动也是有代价的，你可能会错过时机，你也没有机会进步。这个时候，贝叶斯方法给我们提供了一个很好的思路，先做一个预判，动起来，利用新的信息不断修正原来的预判。</p>

<p>2、听人劝、吃饱饭，但又不能听风就是雨。</p>

<p>当我们没有把握时，我们很容易根据新信息调整看法。更大的挑战是，我们已经形成了一个看法，甚至有了成功经验时，当新情况出现后，我们能不能也去调整自己看法。那个黑盒子，我们摸索了一段时间，估计出了里面红球、黑球的概率，但是我们有没有想过，这个黑盒子里的球的比例会变化呢？</p>

<p>有了新信息，我们要对原来的看法做多大程度的修正呢？</p>

<p>这些，不可能有标准答案，但是明白了这个道理，有助于我们及时又谨慎的做出调整。</p>

<p>3、初始概率很重要。</p>

<p>初始概率越准确，我们就能越容易、越快速的得到真实的概率。疑邻盗斧，以貌取人，会让我们离真相越来越远。而如何获得相对靠谱的初始概率，是个硬功夫，它需要你的经验、人脉、平时的深度思考，有时甚至和底层的价值观、思维方式都有关。</p>

<p>丹尼尔.卡尼曼在他的《思考，快与慢》里，就特地强调了初始概率对贝叶斯方法的重要性。</p>

<p>4、对出现的特殊情况要引起足够的重视。</p>

<p>前面我们已经看到了，万分之一概率的事情，也有可能因为特殊事件，一下子变成了50%。所以，每当出现特殊的、罕见的情况时，我们要保持高度警惕，黑盒子里的球的比例是不是变化了？但同时我们也看到，如果检测精度不够高，即便出现了罕见事件，真实概率也可能不到10%。所以，具体要怎么采取行动，还需要进一步观察。</p>

<p>5、信息的收集，信息的质量，以及对信息的判断，是提高决策水平的最重要环节。</p>

<p>只要有新信息，就可以修正，哪怕初始判断错了，新信息足够多，也能修正过来。但是没有信息，就没有修正。所以，在做决定之前，尽可能多的收集信息是必须的。但是错误的信息、低质量的信息，会让你的修正偏离真相越来越远，你能不能区分信息来源的可靠性、能不能进行交叉验证、逻辑推理，就显得至关重要。</p>

<p>要做到这些，甚至某一些，都并不容易，掌握里面的平衡，就更加困难。</p>

<p>所谓高手，就是把自己活成了贝叶斯定理。</p>

<h1 id="数理统计">数理统计</h1>
<p>从样本出发, 估计出总体的分布.</p>

<h3 id="两种方法">两种方法</h3>
<h5 id="参数估计点估计区间估计">参数估计(点估计/区间估计),</h5>
<p>立足于置信区间, 双边</p>

<h5 id="假设检验">假设检验</h5>
<p>对统计量的假设, 立足于小概率.
拒绝域</p>

<h5 id="sig2的无偏估计量">sig^2的无偏估计量</h5>
<ol>
  <li>如果mu已知, 且用s^2来估计,则分母为n</li>
  <li>如果mu未知, 且用x的均值来估计, 则分母为(n-1)</li>
</ol>

<blockquote>
  <p>从已知和未知出发, 主动构造一些量. 严谨的数学公式推导
估计量的评价: 无偏, 有效, 一致(如上述2中分母为n, 在n很大时,是一致的, 但是有偏)</p>
</blockquote>

<ol>
  <li>无偏. 估计量的数学期望等于被估计参数的真实值，则称此此估计量为被估计参数的无偏估计</li>
  <li>有效性就是看估计量的方差值，方差代表波动，波动越小越有效</li>
  <li>一致性就是在大样本条件下，估计值接近真实值。</li>
</ol>
:ET
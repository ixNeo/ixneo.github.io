---
layout:     post   				    # 使用的布局（不需要改）
title:      Q-learning / SARSA			# 标题 
subtitle:  	 
date:       2019-09-09				# 时间
author:     jktian 						# 作者
header-img: img/post-bg-2015.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
tags:								#标签
    - 强化学习
---
# Q-learning
1. 初始化q-table. row为状态，column为动作．选择动作时，通过找值最大的单元格的列坐标
2. 更新显示，通过判断是否终止，选择如何显示. time.sleep()
3. 选取动作：根据epslon>0.9, 随机选取或挑选q-table中最大的动作
4. 得到下一个状态以及由s_i==>s_{i+1}奖励值．if判断，应该可以从r表中得到
> 奖励表是先验的，是人为设置的．q-table是动态更新的
> 奖励是到达某一个状态的值，由到达的状态决定．q-table是联系自身状态得到的综合值

# Q-learning / SARSA / SARSA(lambda)
- 同：
	两者都会根据当前状态，根据epslon和q-table选取动作. 要考虑当前的利益r+下一步的利益gamma*max(q(s_next,a))+自身的状态＋学习率
- 异：
	- 之后．qlearning会根据q-table表，选取下一状态中值最大的那一个（max），去更新q-table的当前状态项,off-policy. 进入下一状态后，执行的动作仍然用epslon-greedy；　
	- sarsa是说到做到，进入下一个状态后，动作不再使用epslon，直接选用上一步确定好的，但是在为此状态选取下一状态时，要用epslon. 相当于，sarsa把q-learning的epslon换为了确定，将其的max换为epslon
q-learning比较冒险拿宝藏最重要．sarsa实际，保命最重要

 - Sarsa(lambda) 　表示走lambda步之后，进行一次更新．用回合更新代替单步更新
 dqn: q-learning+神经网络: 解决state和action表格太大的问题

任何一个求最值的模型，都会引入随机值以避免局部最优解，如模拟退火算法或者此处的epslon-greedy策略


# reward / vlaue
- reward: 可以根据表中某个状态为定值，也可以根据现场的局势判断得到reward, 但一定是短期的
	- 在某个状态下，进行某个动作，会得到多少reward．可以动态得到
	- Reward 定义了强化学习问题中的目标。在每个时间步，环境向agent发送一个称为reward的单个数字。Agent的唯一目标是最大化其长期收到的total reward。
- value: 长期的，基于reward. 有利于最终reward最大化. 综合考虑当前状态和未来状态

如果说reward表明的是在短时间内什么是好的，那么value function则指出从长远来看什么是好的

reward基本上由环境直接给出，但value必须根据agent在其整个生命周期中所做的观察序列来估计和重新估计

参考链接: [莫烦python](https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/2-2-A-q-learning/)

  
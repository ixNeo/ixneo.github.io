
https://morvanzhou.github.io/tutorials/machine-learning/reinforcement-learning/2-2-A-q-learning/
qlearning & sarsa
# qlearning
1. 初始化q-table. row为状态，column为动作．选择动作时，通过找值最大的单元格的列坐标
2. 更新显示，通过判断是否终止，选择如何显示. time.sleep()
3. 选取动作：根据epslon>0.9, 随机选取或挑选q-table中最大的动作
4. 得到下一个状态以及由s_i==>s_{i+1}奖励值．if判断，应该可以从r表中得到
> 奖励表是先验的，是人为设置的．q-table是动态更新的
> 奖励是到达某一个状态的值，由到达的状态决定．q-table是联系自身状态得到的综合值

qlearning和sarsa的异同
同：两者都会根据当前状态，根据epslon和q-table选取动作. 要考虑当前的利益r+下一步的利益gamma*max(q(s_next,a))+自身的状态＋学习率
异：之后．qlearning会根据q-table表，选取下一状态中值最大的那一个（max），去更新q-table的当前状态项,off-policy. 进入下一状态后，执行的动作仍然用epslon-greedy；　
sarsa是说到做到，进入下一个状态后，动作不再使用epslon，直接选用上一步确定好的，但是在为此状态选取下一状态时，要用epslon
相当于，sarsa把q-learning的epslon换为了确定，将其的max换为epslon
q-learning比较冒险拿宝藏最重要．sarsa实际，保命最重要

 Sarsa(lambda) 　表示走lambda步之后，进行一次更新．用回合更新代替单步更新
 dqn: q-learning+神经网络: 解决state和action表格太大的问题

 Reward 定义了强化学习问题中的目标。在每个时间步，环境向agent发送一个称为reward的单个数字。Agent的唯一目标是最大化其长期收到的total reward。

 如果说reward表明的是在短时间内什么是好的，那么value function则指出从长远来看什么是好的

 reward基本上由环境直接给出，但value必须根据agent在其整个生命周期中所做的观察序列来估计和重新估计

# reward&vlaue
reward: 可以根据表中某个状态为定值，也可以根据现场的局势判断得到reward, 但一定是短期的
- 在某个状态下，进行某个动作，会得到多少reward．可以动态得到
  value: 长期的，基于reward. 有利于最终reward最大化. 综合考虑当前状态和未来状态

任何一个求最值的模型，都会引入随机值以避免局部最优解，如模拟退火算法或者此处的epslon-greedy策略

# 缺点

论文中用0.01百分比的间隔表示状态，但真正的状态应该是agent的特征向量本身，建议dqn

如果以离职率作为一种简化的抽象状态

# 神经网络

激活函数:

- 线性:
- 非线性:sigmoid、tanh、relu. sigmoid/tanh比较常见于全连接层，后者relu常见于卷积层.ReLu是神经网络中的一个激活函数，其优于tanh和sigmoid函数
  - **sigmoid函数的功能是相当于把一个实数压缩至0到1之间。当z是非常大的正数时，g(z)会趋近于1，而z是非常小的负数时，则g(z)会趋近于0**。把激活函数看作一种“分类的概率”

CNN

- 输入层(数据清洗/归一化/去均值)==>卷积层(图像窗口和滤波器做内积)==>全连接层(激励函数)/激励层==>池化层(局部区域取最大)==>输出层

- 滤波器filter（带着一组固定权重的神经元）对局部输入数据进行卷积计算。每计算完一个数据窗口内的局部数据后，数据窗口不断平移滑动，直到计算完所有数据. 一个神经元对应一个滤波器.

- 深度/步长/填充值

- 2.引入ReLu的原因

  第一，采用sigmoid等函数，算激活函数时（指数运算），**计算量大**，反向传播求误差梯度时，求导涉及除法，计算量相对大，而采用Relu激活函数，整个过程的计算量节省很多。

  第二，对于深层网络，sigmoid函数反向传播时，很容易就会出现 **梯度消失** 的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0，这种情况会造成信息丢失），从而无法完成深层网络的训练。

  第三，ReLu会使一部分[神经元](https://www.baidu.com/s?wd=%E7%A5%9E%E7%BB%8F%E5%85%83&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)的输出为0，这样就造成了 **网络的稀疏性**，并且减少了参数的相互依存关系，**缓解了过拟合**问题的发生。
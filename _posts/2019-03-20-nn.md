---
layout:     post   				    # 使用的布局（不需要改）
title:     神经网络			# 标题 
subtitle:  	 
date:       2019-03-20				# 时间
author:     jktian 						# 作者
category: 缘因洞--计算机理论
catalog: true 						# 是否归档
tags:								#标签
    - 神经网络

---

* content
{:toc}

# 神经网络

激励函数: elu/relu/linear/prelu/leaky_relu/softplus

优化办法/梯度算法: adam/mom/msprop/sgd(随机梯度下降)

神经网络是一种监督学习,需要大量的样本以及对应标签





# 激活函数:
- 线性:
- 非线性:sigmoid、tanh、relu. sigmoid/tanh比较常见于全连接层，后者relu常见于卷积层.ReLu是神经网络中的一个激活函数，其优于tanh和sigmoid函数
  - sigmoid函数的功能是相当于把一个实数压缩至0到1之间。当z是非常大的正数时，g(z)会趋近于1，而z是非常小的负数时，则g(z)会趋近于0。把激活函数看作一种“分类的概率”

### 引入ReLu的原因

- 采用sigmoid等函数，算激活函数时（指数运算），**计算量大**，反向传播求误差梯度时，求导涉及除法，计算量相对大，而采用Relu激活函数，整个过程的计算量节省很多。
- 对于深层网络，sigmoid函数反向传播时，很容易就会出现 **梯度消失** 的情况（在sigmoid接近饱和区时，变换太缓慢，导数趋于0，这种情况会造成信息丢失），从而无法完成深层网络的训练。
- ReLu会使一部分神经元的输出为0，这样就造成了 **网络的稀疏性**，并且减少了参数的相互依存关系，**缓解了过拟合**问题的发生。

# CNN
### 一般构造
- 输入层, 卷积层,池化层,卷积层,池化层,全连接层(即普通层),全连接层,分类器层
- 输入层(数据清洗/归一化/去均值)==>卷积层(图像窗口和滤波器做内积)==>(激励函数)/激励层==>池化层(局部区域取最大)==>全连接层==>输出层

### 几个概念
- 滤波器filter（带着一组固定权重的神经元）对局部输入数据进行卷积计算。每计算完一个数据窗口内的局部数据后，数据窗口不断平移滑动，直到计算完所有数据. 一个神经元对应一个滤波器.
- 深度/步长/填充值
- 卷积层：对原始图像进行特征提取。
- 最大池化层：没有参数。压缩图像，体积变小。
- 全连接层：把卷积提取的特征组合在一起，用组合到一起的特征再进行分类。
	- 全连接的目的:因为传统的网络我们的输出都是分类，也就是几个类别的概率甚至就是一个数--类别号，那么全连接层就是高度提纯的特征了，方便交给最后的分类器或者回归。

前面的卷积和池化相当于做特征工程，后面的全连接相当于做特征加权。

# DQN
- experience replay: 存储数据,然后随机采样,进行训练/梯度下降
- 和qlearning的联系: 输入状态数据,输出所有行为的q值. 目标函数/loss是q_target, qlearning为deep-learning提供了损失函数
缺点: 训练时间长, 通用性有限
特点:输入维度多,输出维度少
### 步骤
fixed-value. dqn中有两个结构相同的网络,一个用于实时输出不同action的q_value,存的是现值(估计值), 一个用于存储target值,存下一个状态.每隔一段时间被更新一次. 两个网络的出现是qlearning公式的必然
存储经验,
大的框架仍然是qlearning, 只是每隔几步,神经网络学习一次

# RNN
LSTM属于循环神经网络

# 过拟合的解决方案
1. 加大数据量
2. 正规化,对过大或过小的w进行惩罚
3. dropout, 随机去掉一些神经元,避免对某些神经元的过度依赖

# tensorflow框架
### 术语
（1）batchsize：批大小。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；
（2）iteration：1个iteration等于使用batchsize个样本训练一次；
（3）epoch：1个epoch等于使用训练集中的全部样本训练一次；

~~~

~~~
---

layout:     post   				    # 使用的布局（不需要改）
title:      n-gram模型			# 标题 
subtitle:  	 
date:       2019-03-27				# 时间
author:     jktian 						# 作者
header-img: img/post-bg-2015.jpg 	#这篇文章标题背景图片
catalog: true 						# 是否归档
category: 缘因洞--计算机理论
tags:								#标签
    - log
    - nlp
    - kg

---
* content
{:toc}
# N-gram

N-Gram是基于一个假设：第n个词出现与前n-1个词相关，而与其他任何词不相关。（这也是隐马尔可夫当中的假设。）整个句子出现的概率就等于各个词出现的概率乘积。各个词的概率可以通过语料中统计计算得到。考虑句子中单词之间的顺序.


朴素贝叶斯将句子处理为一个**词袋模型（Bag-of-Words, BoW）**，以至于不考虑每个单词的顺序

N-gram模型是一种语言模型（Language Model，LM），语言模型是一个基于概率的判别模型，它的输入是一句话（单词的顺序序列），输出是这句话的概率，即这些单词的联合概率（joint probability）.条件概率用频数/频数计算得到

常用的有 Bi-gram (N=2N=2) 和 Tri-gram (N=3N=3)，

具有稀疏性,需要进行数据平滑处理,不然容易为0

对训练数据集要求较高

Word2Vec解决的问题已经和上面讲到的N-gram、NNLM等不一样了，它要做的事情是：学习一个从高维稀疏离散向量到低维稠密连续向量的映射。该映射的特点是，近义词向量的欧氏距离比较小，词向量之间的加减法有实际物理意义。Word2Vec由两部分组成：CBoW和Skip-Gram。其中CBoW的结构很简单，在NNLM的基础上去掉隐层，Embedding层直接连接到Softmax，CBoW的输入是某个Word的上下文（例如前两个词和后两个词），Softmax的输出是关于当前词的某个概率，即CBoW是从上下文到当前词的某种映射或者预测。Skip-Gram则是反过来，从当前词预测上下文，至于为什么叫Skip-Gram这个名字，原因是在处理过程中会对词做采样